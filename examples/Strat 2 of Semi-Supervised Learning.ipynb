{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports + Path Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from torch import optim\n",
    "from time import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile2vec_dir = '/home/agupta21/gcloud/231n_gitproject' # Uncomment for anshul\n",
    "# tile2vec_dir = '/home/shailimonchik/project' # Uncomment for Shai\n",
    "sys.path.append('../')\n",
    "sys.path.append(tile2vec_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets import TileTripletsDataset, GetBands, RandomFlipAndRotate, ClipAndScale, ToFloatTensor, triplet_dataloader\n",
    "from src.tilenet import make_tilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training import prep_triplets, train_triplet_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "from src.resnet import ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Classification\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader + TileNet Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Environment stuff\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "cuda = torch.cuda.is_available()\n",
    "print(cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Parameters\n",
    "img_type = 'naip'\n",
    "tile_dir = '/home/agupta21/gcloud/231n_gitproject/data/triplets/'\n",
    "bands = 4\n",
    "augment = False\n",
    "batch_size = 50\n",
    "shuffle = True\n",
    "num_workers = 4\n",
    "n_triplets = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader set up complete.\n"
     ]
    }
   ],
   "source": [
    "dataloader = triplet_dataloader(img_type, tile_dir, bands=bands, augment=augment,\n",
    "                                batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, \n",
    "                                n_triplets=n_triplets, pairs_only=True)\n",
    "print('Dataloader set up complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n",
      "65539\n"
     ]
    }
   ],
   "source": [
    "#generate list\n",
    "random.seed(21)\n",
    "randList = []\n",
    "while len(randList)!= 10000:\n",
    "    randIndex = random.randint(0,99999)\n",
    "    if randIndex not in randList:\n",
    "        randList.append(randIndex)\n",
    "print(len(randList))\n",
    "\n",
    "randSet = set(randList)\n",
    "print(len(randSet))\n",
    "print(list(randSet)[0]) #65339"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tripletsToLabels = {}\n",
    "# for idx in range(65538,100000):\n",
    "#     if idx in randSet:\n",
    "#         #do shit\n",
    "#         part1 = \"../data/triplets/\" + str(idx)\n",
    "#         anchor = np.load(part1 + \"anchor.npy\")\n",
    "#         neighbor = np.load(part1 + \"neighbor.npy\")\n",
    "#         distant = np.load(part1 + \"distant.npy\")\n",
    "#         counts_anchor = np.bincount(anchor[:,:,4].reshape(2500,))\n",
    "#         anchor_label = np.argmax(counts_anchor)\n",
    "#         counts_nei = np.bincount(neighbor[:,:,4].reshape(2500,))\n",
    "#         neighbor_label = np.argmax(counts_nei)\n",
    "#         counts_distant = np.bincount(distant[:,:,4].reshape(2500,))\n",
    "#         distant_label = np.argmax(counts_distant)\n",
    "#         tripletsToLabels[idx] = (anchor_label, neighbor_label, distant_label)\n",
    "#     if idx == 65539:\n",
    "#         print(anchor_label,neighbor_label,distant_label)\n",
    "        \n",
    "#         print('Key is {}, Labels are {}'.format(idx, tripletsToLabels[idx]))\n",
    "#     if idx % 10000 == 0:\n",
    "#         print('idx is {}'.format(idx))\n",
    "# print(len(tripletsToLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Get set of all values:\n",
    "# allVals = set()\n",
    "# for idx in range(100000):\n",
    "#     if idx % 1000 == 0:\n",
    "#         print(idx)\n",
    "#     part1 = \"../data/triplets/\" + str(idx)\n",
    "#     anchor = np.load(part1 + \"anchor.npy\")\n",
    "#     neighbor = np.load(part1 + \"neighbor.npy\")\n",
    "#     distant = np.load(part1 + \"distant.npy\")\n",
    "#     counts_anchor = np.bincount(anchor[:,:,4].reshape(2500,))\n",
    "#     anchor_label = np.argmax(counts_anchor)\n",
    "#     counts_nei = np.bincount(neighbor[:,:,4].reshape(2500,))\n",
    "#     neighbor_label = np.argmax(counts_nei)\n",
    "#     counts_distant = np.bincount(distant[:,:,4].reshape(2500,))\n",
    "#     distant_label = np.argmax(counts_distant)\n",
    "#     allVals.add(anchor_label)\n",
    "#     allVals.add(neighbor_label)\n",
    "#     allVals.add(distant_label)\n",
    "# print(len(allVals))\n",
    "# print(allVals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 2, 3, 131, 4, 12, 141, 142, 21, 22, 23, 24, 152, 27, 28, 33, 36, 37, 41, 42, 43, 44, 176, 49, 48, 53, 54, 59, 61, 190, 66, 67, 195, 69, 71, 72, 74, 75, 76, 205, 204, 206, 208, 77, 209, 211, 212, 213, 216, 217, 218, 220, 225, 226, 227, 236, 237, 111, 242, 121, 122, 123, 124}\n",
      "[ 0  1  2 38  3  4 39 40  5  6  7  8 41  9 10 11 12 13 14 15 16 17 42 19\n",
      " 18 20 21 22 23 43 24 25 44 26 27 28 29 30 31 46 45 47 48 32 49 50 51 52\n",
      " 53 54 55 56 57 58 59 60 61 33 62 34 35 36 37]\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62}\n"
     ]
    }
   ],
   "source": [
    "#Transform the dict: \n",
    "allVals = {1, 2, 131, 4, 3, 12, 141, 142, 21, 22, 23, 24, 152, 27, 28, 33, 36, 37, 41, 42, 43, 44, 176, 49, 48, 53, 54, 59, 61, 190, 66, 67, 195, 69, 71, 72, 74, 75, 76, 205, 204, 206, 208, 77, 209, 211, 212, 213, 216, 217, 218, 220, 225, 226, 227, 236, 237, 111, 242, 121, 122, 123, 124}\n",
    "print(set(allVals))\n",
    "le = LabelEncoder()\n",
    "transformedLabels = le.fit_transform(list(allVals))\n",
    "print(transformedLabels)\n",
    "print(set(transformedLabels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tile_dir = '../data/tiles'\n",
    "# n_tiles = 1000\n",
    "# y = np.load(os.path.join(tile_dir, 'y.npy'))\n",
    "# print(y.shape)\n",
    "# print(set(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in set(y):\n",
    "#     val = int(i)\n",
    "#     if val not in allVals:\n",
    "#         print(\"Crushing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #DO NOT RUN THIS ANYMORE\n",
    "# f = open('tripletsToLabels2', 'w')\n",
    "# f.write(str(tripletsToLabels))\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "(123, 69, 69)\n"
     ]
    }
   ],
   "source": [
    "tempDict = eval(open('tripletsToLabels', 'r').read())\n",
    "print(len(tempDict))\n",
    "\n",
    "print(tempDict[65539])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    }
   ],
   "source": [
    "#DONT RUN THIS AGAIN!!!!! (actually you can )\n",
    "for key, value in tempDict.items():\n",
    "    value2 = le.transform(value)\n",
    "    tempDict[key] = value2\n",
    "maxVal = 0\n",
    "for key, value in tempDict.items():\n",
    "    maxP = max(value)\n",
    "    if maxP > maxVal:\n",
    "        maxVal = maxP\n",
    "print(maxVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = bands\n",
    "z_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TileNet set up complete.\n"
     ]
    }
   ],
   "source": [
    "TileNet = make_tilenet(in_channels=in_channels, z_dim=z_dim, strat2=True, dictionary_labels=tempDict, idx_include=randSet)\n",
    "TileNet.train()\n",
    "if cuda: TileNet.cuda()\n",
    "print('TileNet set up complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter \n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(TileNet.parameters(), lr=lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model + Writing Each Epoch to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 12\n",
    "margin = 10\n",
    "l2 = 0.01\n",
    "print_every = 10000\n",
    "save_models = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/home/agupta21/gcloud/231n_gitproject/models/'\n",
    "if not os.path.exists(model_dir): os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training.................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../src/tilenet.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = softy(score_upd)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: [10000/100000 (10%)], Avg loss: 11.0158\n",
      "Epoch 1: [30000/100000 (30%)], Avg loss: 9.7622\n",
      "Epoch 1: [40000/100000 (40%)], Avg loss: 8.5537\n",
      "Epoch 1: [50000/100000 (50%)], Avg loss: 7.2448\n",
      "Epoch 1: [60000/100000 (60%)], Avg loss: 8.2030\n",
      "Epoch 1: [70000/100000 (70%)], Avg loss: 7.7018\n",
      "Epoch 1: [80000/100000 (80%)], Avg loss: 7.9241\n",
      "Epoch 1: [90000/100000 (90%)], Avg loss: 8.3396\n",
      "Epoch 1: [100000/100000 (100%)], Avg loss: 7.4326\n",
      "Finished epoch 1: 2332.901s\n",
      "  Average loss: 8.6013\n",
      "  Average l_n: 3.8908\n",
      "  Average l_d: -13.3800\n",
      "  Average l_nd: -9.4891\n",
      "\n",
      "Epoch 2: [10000/100000 (10%)], Avg loss: 7.3999\n",
      "Epoch 2: [20000/100000 (20%)], Avg loss: 7.7987\n",
      "Epoch 2: [30000/100000 (30%)], Avg loss: 7.4635\n",
      "Epoch 2: [40000/100000 (40%)], Avg loss: 7.3958\n",
      "Epoch 2: [50000/100000 (50%)], Avg loss: 6.5507\n",
      "Epoch 2: [60000/100000 (60%)], Avg loss: 7.5323\n",
      "Epoch 2: [70000/100000 (70%)], Avg loss: 6.7191\n",
      "Epoch 2: [80000/100000 (80%)], Avg loss: 7.2707\n",
      "Epoch 2: [90000/100000 (90%)], Avg loss: 7.8020\n",
      "Epoch 2: [100000/100000 (100%)], Avg loss: 6.9612\n",
      "Finished epoch 2: 4665.271s\n",
      "  Average loss: 7.2894\n",
      "  Average l_n: 3.7600\n",
      "  Average l_d: -13.7806\n",
      "  Average l_nd: -10.0206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "#with open(results_fn, 'w') as file:\n",
    "\n",
    "total_list = []\n",
    "small_list = []\n",
    "print('Begin training.................')\n",
    "for epoch in range(0, epochs):\n",
    "    (avg_loss, avg_small,avg_l_n, avg_l_d, avg_l_nd) = train_triplet_epoch(\n",
    "        TileNet, cuda, dataloader, optimizer, epoch+1, margin=margin, l2=l2,\n",
    "        print_every=print_every, t0=t0)\n",
    "    total_list.append((epoch+1,avg_loss))\n",
    "    small_list.append((epoch+1,avg_small))\n",
    "    append_name = \"strat2.0_ep\" + str(epoch+1) + \".ckpt\"\n",
    "    if save_models:\n",
    "        model_fn = os.path.join(model_dir,append_name)\n",
    "        torch.save(TileNet.state_dict(),model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_list)\n",
    "print(small_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Process Y Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note to self: need to embed tiles first and then figure out loop for reading in the saved model and plotting\n",
    "#the classification accuracies per epoch\n",
    "tile_dir = '../data/tiles'\n",
    "n_tiles = 1000\n",
    "y = np.load(os.path.join(tile_dir, 'y.npy'))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CDL classes\n",
    "print(set(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = LabelEncoder().fit_transform(y)\n",
    "print(set(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Epochs' Weights + Run each on tile embeddings + Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up model\n",
    "in_channels = 4\n",
    "z_dim = 512\n",
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTileEmbeddings(tilenet):\n",
    "    X = np.zeros((n_tiles, z_dim))\n",
    "    for idx in range(n_tiles):\n",
    "        tile = np.load(os.path.join(tile_dir, '{}tile.npy'.format(idx+1)))\n",
    "        # Get first 4 NAIP channels (5th is CDL mask)\n",
    "        tile = tile[:,:,:4]\n",
    "        # Rearrange to PyTorch order\n",
    "        tile = np.moveaxis(tile, -1, 0)\n",
    "        tile = np.expand_dims(tile, axis=0)\n",
    "        # Scale to [0, 1]\n",
    "        tile = tile / 255\n",
    "        # Embed tile\n",
    "        tile = torch.from_numpy(tile).float()\n",
    "        tile = Variable(tile)\n",
    "        if cuda: tile = tile.cuda()\n",
    "        z = tilenet.encode(tile)\n",
    "        if cuda: z = z.cpu()\n",
    "        z = z.data.numpy() #1 by 512\n",
    "        X[idx,:] = z\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochsToAccuracy = []\n",
    "epochsToSTD = []\n",
    "for i in range(0,epochs): #iterator\n",
    "    curEpoch = i + 1\n",
    "    # Setting up model\n",
    "    tilenet = ResNet18()\n",
    "    if cuda: tilenet.cuda()\n",
    "    model_fn = \"../models/test1_ep\"+str(curEpoch)+\".ckpt\" #open file\n",
    "    #checkpoint = torch.load(model_fn)\n",
    "    tilenet.load_state_dict(torch.load(model_fn), strict=False)\n",
    "    #tilenet.load_state_dict(checkpoint)\n",
    "    tilenet.eval()\n",
    "    \n",
    "    X = getTileEmbeddings(tilenet) #function above\n",
    "    \n",
    "    #train random forest classifier\n",
    "    n_trials = 100\n",
    "    accs = np.zeros((n_trials,))\n",
    "    for i in range(n_trials):\n",
    "        # Splitting data and training RF classifer\n",
    "        X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2)\n",
    "        rf = RandomForestClassifier()\n",
    "        rf.fit(X_tr, y_tr) #X-tr is 512 by 1\n",
    "        accs[i] = rf.score(X_te, y_te)\n",
    "    print(\"Results for Epoch Number: \", str(curEpoch))\n",
    "    print('Mean accuracy: {:0.4f}'.format(accs.mean()))\n",
    "    print('Standard deviation: {:0.4f}'.format(accs.std()))\n",
    "    epochsToAccuracy.append((curEpoch,accs.mean()))\n",
    "    epochsToSTD.append((curEpoch,accs.std()))\n",
    "print(epochsToAccuracy)\n",
    "print(epochsToSTD)\n",
    "    #save value\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epochsToSTD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(*zip(*epochsToAccuracy))\n",
    "plt.title('Triplet Augmentation Average Test Performance')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "print(epochsToAccuracy[9][1], \"was the Max Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(*zip(*total_list))\n",
    "plt.title('Triplet Augmentation Loss Performance')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "print(\"The min loss was\",str(total_list[9][1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(*zip(*epochsToSTD))\n",
    "plt.title('STD on Random Forest Test Prediction')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Standard Deviation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
